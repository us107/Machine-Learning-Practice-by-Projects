# Polynomial Regression

> **TL;DR:** Polynomial regression is not a new algorithm â€” it's linear regression with engineered features.

---

## What Is It?

Polynomial regression extends linear regression to fit **curved, non-linear data** by adding polynomial terms (xÂ², xÂ³, ...) as new features, then running the same old linear regression on top of them.

### The Math

You're still minimizing the same loss function:

$$J(\theta) = \frac{1}{n}\sum (y - \hat{y})^2$$

The only thing that changes is the **shape of the hypothesis** â€” not the algorithm.

| Linear Regression | Polynomial Regression |
|---|---|
| Å· = Î¸â‚€ + Î¸â‚x | Å· = Î¸â‚€ + Î¸â‚x + Î¸â‚‚xÂ² + Î¸â‚ƒxÂ³ ... |
| Fits a straight line | Fits a curve |
| Same loss function âœ… | Same loss function âœ… |
| Same solver âœ… | Same solver âœ… |

---

## How It Works (2 Steps)

**Step 1 â€” Feature Engineering:** Transform your input `x` into `[x, xÂ², xÂ³, ...]`

**Step 2 â€” Linear Regression:** Train a regular linear regression model on the expanded features

That's it. No new model. No new optimizer.

---

## Why Use It?

- Your data has a **curved or non-linear trend** that a straight line can't capture
- You want the **simplicity and interpretability** of linear regression, but with more flexibility
- You need a **lightweight model** without jumping to complex algorithms like neural networks or SVMs
- You want **full control** over model complexity by choosing the polynomial degree

---

## Where to Use It?

| Domain | Example Use Case |
|---|---|
| ğŸ“ˆ Finance | Modeling diminishing returns on investment |
| ğŸŒ¡ï¸ Science & Physics | Projectile motion, temperature curves |
| ğŸ  Real Estate | Price vs. square footage (non-linear relationship) |
| ğŸ¥ Healthcare | Drug dosage-response curves |
| ğŸŒ± Agriculture | Crop yield vs. fertilizer amount |
| ğŸš— Engineering | Speed vs. fuel consumption (U-shaped curve) |
| ğŸ“Š Economics | Supply/demand curves, GDP trends |

**Good fit when:**
- Your residuals from linear regression show a clear pattern
- You have a relatively small, clean dataset
- The relationship is curved but not wildly complex

**Not ideal when:**
- Data is high-dimensional (polynomial features explode in count)
- You need very high degrees (risk of overfitting)
- Better handled by tree-based models or neural nets

---

## Key Takeaways

- âœ… Polynomial regression **handles curved data** that linear regression can't
- âš ï¸ Higher degree **â‰ ** better â€” overfitting becomes a serious risk
- ğŸ” It uses the **same linear regression algorithm** under the hood
- ğŸ§  Polynomial features simply **change the hypothesis space** (what shapes the model can learn)

---

## Overfitting Warning

```
Degree 1  â†’  Underfit   (too simple, misses the curve)
Degree 3  â†’  Just right (captures the trend)
Degree 15 â†’  Overfit    (memorizes noise, fails on new data)
```

Always use **cross-validation** to pick the right degree. A model that fits training data perfectly is often the worst model for real-world predictions.

---

## Quick Code Example (Python)

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline

degree = 3
model = make_pipeline(PolynomialFeatures(degree), LinearRegression())
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

## Summary

Polynomial regression is one of the best tools to reach for when your data is curved but you want to keep things simple and explainable. It's a reminder that **feature engineering often matters more than model choice**.